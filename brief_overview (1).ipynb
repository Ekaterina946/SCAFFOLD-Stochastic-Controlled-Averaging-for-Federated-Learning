{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Проблема и примеры возникновения\n",
        "\n",
        "Современные мобильные устройства имеют доступ к обширным данным, пригодным для обучения моделей, что в свою очередь может значительно улучшить пользовательский опыт на устройстве. Например, языковые модели могут улучшить распознавание речи и ввод текста, а модели изображений могут автоматически выбирать хорошие фотографии. Однако эти богатые данные часто являются конфиденциальными, имеют большой объем или и то, и другое, что может исключить возможность регистрации в центре обработки данных и обучения там с использованием традиционных методов.\n",
        "\n",
        "\n",
        "### Федеративное обучение\n",
        "\n",
        "Альтернативный подход, который предполагает оставление распределенных данных для обучения на мобильных устройствах и обучение общей модели путем агрегирования локальных вычисленных обновлений.\n",
        "\n",
        "### Более конкретные свойства задач для федеративного обучения:\n",
        "\n",
        "1)Обучение на реальных данных с мобильных устройств предоставляет явное преимущество перед обучением на заменительных данных, которые обычно доступны в центре обработки данных.\n",
        "\n",
        "2)Эти данные являются конфиденциальными или имеют большой размер (по сравнению с размером модели), поэтому предпочтительно не регистрировать их в центре данных исключительно для целей обучения модели (в соответствии с принципом фокусированной коллекции).\n",
        "\n",
        "3)Для задач с учителем, метки на данных могут естественно выводиться из взаимодействия пользователя.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x7LKE7ypOt1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Постановка задачи:\n",
        "\n",
        "Задача: минимизация суммы стохастических функций, имея доступ только к стохастическим выборкам:\n",
        "\n",
        "$$\\min\\limits_{x \\in R^d} \\{ f(x) := \\frac{1}{N} \\sum\\limits_{i=1}^{N}(f_i(x) := \\mathbb{E}_{\\zeta_i} [f_i(x, \\zeta_i)])\\}$$\n",
        "\n",
        "Функции $f_i$ представляют функцию потерь на клиенте. Все наши результаты могут быть легко расширены на случай взвешенных функций.\n",
        "\n",
        "Предполагаем, что функция $f$ ограничена снизу значением $f^*$, а функция $f_i$ является $\\beta$-гладкой. Кроме того, предполагаем, что $g_i(x) := \\nabla f_i(x; \\zeta_i)$ является несмещенным стохастическим градиентом $f_i$ с дисперсией, ограниченной $\\sigma^2$. Для некоторых результатов мы предполагаем, что $\\mu \\geq 0$ (сильная) выпуклость. $\\sigma$ ограничивает дисперсию внутри клиентов. Вводим два термина, нестандартные для данного контекста.\n",
        "\n",
        "**Условие 1:** (G, B)-BGD, или ограниченная диссимиларность градиента: существуют константы $G \\geq 0$ и $B \\geq 1$ такие, что\n",
        "\n",
        "$\n",
        "\\frac{1}{N} \\sum_{i=1}^{N} \\lVert \\nabla f_i(x) \\rVert^2 \\leq G^2 + B^2 \\lVert \\nabla f(x) \\rVert^2, \\quad \\forall x.\n",
        "$\n",
        "\n",
        "Если $f_i$ являются выпуклыми, мы можем усилить предположение до\n",
        "\n",
        "$\n",
        "\\frac{1}{N} \\sum_{i=1}^{N} \\lVert \\nabla f_i(x) \\rVert^2 \\leq G^2 + 2\\beta B^2 (f(x) - f^*), \\quad \\forall x.\n",
        "$\n",
        "\n",
        "**Условие 2:** $\\delta$-BHD, или ограниченная диссимиларность гессиана:\n",
        "\n",
        "$\n",
        "\\lVert \\nabla^2 f_i(x) - \\nabla^2 f(x) \\rVert \\leq \\delta, \\quad \\forall x.\n",
        "$\n",
        "\n",
        "Кроме того, $f_i$ является $\\delta$-слабо выпуклой, то есть $\\nabla^2 f_i(x) \\succeq -\\delta I$.\n",
        "\n",
        "Предположения из условия 1 и 2 ортогональны — возможно иметь $G = 0$ и $\\delta = 2\\beta$, или $\\delta = 0$, но $G > 1$."
      ],
      "metadata": {
        "id": "IZJthADfOFFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Основная идея статьи\n",
        "\n",
        "В статье рассказывается про новый алгоритм для федеративного обучения SCAFFOLD, доказывается его превосходство над прошлым решением - алгоритмом FEDAVG.\n",
        "\n",
        "FEDAVG стал алгоритмом выбора для федеративного обучения из-за своей простоты и низкой стоимости коммуникации. Однако, его производительность не полностью понята. Были получены точные скорости сходимости для FEDAVG и доказано, что он страдает от \"дрейфа клиента\"(модель, обученная на данных одного участника (клиента), показывает низкую эффективность или неустойчивость при применении к данным другого клиента), когда данные имеют различные характеристики, структуры или распределения в разных частях набора данных, что приводит к нестабильной и медленной сходимости.\n",
        "\n",
        "В статье же в качестве решения предлагается новый алгоритм SCAFFOLD, который использует управляемые переменные (снижение дисперсии), чтобы скорректировать \"дрейф клиента\" в его локальных обновлениях.\n",
        "\n",
        "Кроме того, в статье доказывается, что SCAFFOLD требует значительно меньше раундов коммуникации и не подвержен проблемам на данных, которые имеют различные характеристики, структуры или распределения в разных частях набора.\n",
        "\n",
        "Также SCAFFOLD для квадратичных функций может воспользоваться схожестью данных клиента, что приводит к еще более быстрой сходимости."
      ],
      "metadata": {
        "id": "s6CV4RnzSPXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Еще немного про федеративное обучение и еще один пример использовани(это описание и пример взяты [отсюда](https://habr.com/ru/companies/piter/articles/458800/))\n",
        "\n",
        "Идея федеративного обучения зародилась из того, что многие данные, содержащие полезную информацию для решения задач (например, для диагностики онкологических заболеваний с использованием МРТ), трудно получить в количествах, достаточных для обучения мощной модели глубокого обучения. Кроме полезной информации, необходимой для обучения модели, наборы данных содержат также другие сведения, не имеющие отношения к решаемой задаче, но их раскрытие кому-либо потенциально может нанести вред.\n",
        "\n",
        "Федеративное обучение — это методика заключения модели в защищенную среду и ее обучение без перемещения данных куда-либо. Рассмотрим пример.\n",
        "\n",
        "\n",
        "#### Обучаем выявлять спам\n",
        "\n",
        "Допустим, нам нужно обучить модель определять спам по электронным письмам людей\n",
        "\n",
        "В данном случае мы говорим о классификации электронной почты. Нашу первую модель мы обучим на общедоступном наборе данных с названием Enron. Это огромный корпус электронных писем, опубликованных в ходе слушаний по делу компании Enron (теперь это стандартный аналитический корпус электронной почты)"
      ],
      "metadata": {
        "id": "pxDtKj9jgoM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Давайте реализуем дальше код этой задачи для выявления спама:\n",
        "\n"
      ],
      "metadata": {
        "id": "tg16tWCFjgcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "import sys\n",
        "import codecs\n",
        "\n",
        "np.random.seed(12345)\n",
        "\n",
        "with codecs.open('spam.txt',\"r\",encoding='utf-8',errors='ignore') as f:\n",
        "      raw = f.readlines()\n",
        "\n",
        "vocab, spam, ham = (set([\"<unk>\"]), list(), list())\n",
        "for row in raw:\n",
        "     spam.append(set(row[:-2].split(\" \")))\n",
        "     for word in spam[-1]:\n",
        "          vocab.add(word)\n",
        "\n",
        "with codecs.open('ham.txt',\"r\",encoding='utf-8',errors='ignore') as f:\n",
        "     raw = f.readlines()\n",
        "\n",
        "for row in raw:\n",
        "     ham.append(set(row[:-2].split(\" \")))\n",
        "     for word in ham[-1]:\n",
        "          vocab.add(word)\n",
        "\n",
        "vocab, w2i = (list(vocab), {})\n",
        "for i,w in enumerate(vocab):\n",
        "     w2i[w] = i\n",
        "\n",
        "def to_indices(input, l=500):\n",
        "    indices = list()\n",
        "    for line in input:\n",
        "          if(len(line) < l):\n",
        "              line = list(line) + [\"<unk>\"] * (l - len(line))\n",
        "              idxs = list()\n",
        "              for word in line:\n",
        "                   idxs.append(w2i[word])\n",
        "              indices.append(idxs)\n",
        "    return indices"
      ],
      "metadata": {
        "id": "5xopxoIwmLUT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_idx = to_indices(spam)\n",
        "ham_idx = to_indices(ham)\n",
        "\n",
        "train_spam_idx = spam_idx[0:-1000]\n",
        "train_ham_idx = ham_idx[0:-1000]\n",
        "\n",
        "test_spam_idx = spam_idx[-1000:]\n",
        "test_ham_idx = ham_idx[-1000:]\n",
        "\n",
        "train_data = list()\n",
        "train_target = list()\n",
        "\n",
        "test_data = list()\n",
        "test_target = list()\n",
        "\n",
        "for i in range(max(len(train_spam_idx),len(train_ham_idx))):\n",
        "     train_data.append(train_spam_idx[i%len(train_spam_idx)])\n",
        "     train_target.append([1])\n",
        "\n",
        "     train_data.append(train_ham_idx[i%len(train_ham_idx)])\n",
        "     train_target.append([0])\n",
        "\n",
        "for i in range(max(len(test_spam_idx),len(test_ham_idx))):\n",
        "     test_data.append(test_spam_idx[i%len(test_spam_idx)])\n",
        "     test_target.append([1])\n",
        "\n",
        "     test_data.append(test_ham_idx[i%len(test_ham_idx)])\n",
        "     test_target.append([0])\n",
        "\n",
        "def train(model, input_data, target_data, batch_size=500, iterations=5):\n",
        "     n_batches = int(len(input_data) / batch_size)\n",
        "     for iter in range(iterations):\n",
        "          iter_loss = 0\n",
        "          for b_i in range(n_batches):\n",
        "\n",
        "               model.weight.data[w2i['<unk>']] *= 0\n",
        "               input = Tensor(input_data[b_i*bs:(b_i+1)*bs], autograd=True)\n",
        "               target = Tensor(target_data[b_i*bs:(b_i+1)*bs], autograd=True)\n",
        "\n",
        "               pred = model.forward(input).sum(1).sigmoid()\n",
        "               loss = criterion.forward(pred,target)\n",
        "               loss.backward()\n",
        "               optim.step()\n",
        "\n",
        "               iter_loss += loss.data[0] / bs\n",
        "\n",
        "               sys.stdout.write(\"\\r\\tLoss:\" + str(iter_loss / (b_i+1)))\n",
        "          print()\n",
        "     return model\n",
        "\n",
        "def test(model, test_input, test_output):\n",
        "\n",
        "     model.weight.data[w2i['<unk>']] *= 0\n",
        "\n",
        "     input = Tensor(test_input, autograd=True)\n",
        "     target = Tensor(test_output, autograd=True)\n",
        "     pred = model.forward(input).sum(1).sigmoid()\n",
        "     return ((pred.data > 0.5) == target.data).mean()"
      ],
      "metadata": {
        "id": "ix8IPmSvnISy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделаем модель федеративной\n",
        "\n",
        "Выше было выполнено самое обычное глубокое обучение. Теперь добавим конфиденциальности\n",
        "\n",
        "В предыдущем разделе мы реализовали пример анализа электронной почты. Теперь поместим все электронные письма в одно место. Это старый добрый метод работы (который все еще широко используется во всем мире). Для начала сымитируем окружение федеративного обучения, в котором имеется несколько разных коллекций писем:\n"
      ],
      "metadata": {
        "id": "RF1JX7CAug6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bob = (train_data[0:1500], train_target[0:1500])\n",
        "alice = (train_data[1500:], train_target[1500:])"
      ],
      "metadata": {
        "id": "kzb-8WwWujxO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdswtfhpv8QY",
        "outputId": "98d6343c-2127-471d-e428-d63c1db68bd8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "model = nn.Embedding(len(vocab), 1)\n",
        "model.weight.data *= 0\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "WcCcdsxlvFFz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте теперь реализуем собственно сам наш метод оптимизации\n",
        "\n",
        "Будем использовать torch для написания"
      ],
      "metadata": {
        "id": "awPXklg3xFxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Optimizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "4waaNCiGxg-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aGW0IEdo02sx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}