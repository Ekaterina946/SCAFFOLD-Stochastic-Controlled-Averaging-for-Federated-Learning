{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Проблема и примеры возникновения\n",
        "\n",
        "Современные мобильные устройства имеют доступ к обширным данным, пригодным для обучения моделей, что в свою очередь может значительно улучшить пользовательский опыт на устройстве. Например, языковые модели могут улучшить распознавание речи и ввод текста, а модели изображений могут автоматически выбирать хорошие фотографии. Однако эти богатые данные часто являются конфиденциальными, имеют большой объем или и то, и другое, что может исключить возможность регистрации в центре обработки данных и обучения там с использованием традиционных методов.\n",
        "\n",
        "\n",
        "### Федеративное обучение\n",
        "\n",
        "Альтернативный подход, который предполагает оставление распределенных данных для обучения на мобильных устройствах и обучение общей модели путем агрегирования локальных вычисленных обновлений.\n",
        "\n",
        "### Более конкретные свойства задач для федеративного обучения:\n",
        "\n",
        "1)Обучение на реальных данных с мобильных устройств предоставляет явное преимущество перед обучением на заменительных данных, которые обычно доступны в центре обработки данных.\n",
        "\n",
        "2)Эти данные являются конфиденциальными или имеют большой размер (по сравнению с размером модели), поэтому предпочтительно не регистрировать их в центре данных исключительно для целей обучения модели (в соответствии с принципом фокусированной коллекции).\n",
        "\n",
        "3)Для задач с учителем, метки на данных могут естественно выводиться из взаимодействия пользователя.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x7LKE7ypOt1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Постановка задачи:\n",
        "\n",
        "Задача: минимизация суммы стохастических функций, имея доступ только к стохастическим выборкам:\n",
        "\n",
        "$$\\min\\limits_{x \\in R^d} \\{ f(x) := \\frac{1}{N} \\sum\\limits_{i=1}^{N}(f_i(x) := \\mathbb{E}_{\\zeta_i} [f_i(x, \\zeta_i)])\\}$$\n",
        "\n",
        "Функции $f_i$ представляют функцию потерь на клиенте. Все наши результаты могут быть легко расширены на случай взвешенных функций.\n",
        "\n",
        "Предполагаем, что функция $f$ ограничена снизу значением $f^*$, а функция $f_i$ является $\\beta$-гладкой. Кроме того, предполагаем, что $g_i(x) := \\nabla f_i(x; \\zeta_i)$ является несмещенным стохастическим градиентом $f_i$ с дисперсией, ограниченной $\\sigma^2$. Для некоторых результатов мы предполагаем, что $\\mu \\geq 0$ (сильная) выпуклость. $\\sigma$ ограничивает дисперсию внутри клиентов. Вводим два термина, нестандартные для данного контекста.\n",
        "\n",
        "**Условие 1:** (G, B)-BGD, или ограниченная диссимиларность градиента: существуют константы $G \\geq 0$ и $B \\geq 1$ такие, что\n",
        "\n",
        "$\n",
        "\\frac{1}{N} \\sum_{i=1}^{N} \\lVert \\nabla f_i(x) \\rVert^2 \\leq G^2 + B^2 \\lVert \\nabla f(x) \\rVert^2, \\quad \\forall x.\n",
        "$\n",
        "\n",
        "Если $f_i$ являются выпуклыми, мы можем усилить предположение до\n",
        "\n",
        "$\n",
        "\\frac{1}{N} \\sum_{i=1}^{N} \\lVert \\nabla f_i(x) \\rVert^2 \\leq G^2 + 2\\beta B^2 (f(x) - f^*), \\quad \\forall x.\n",
        "$\n",
        "\n",
        "**Условие 2:** $\\delta$-BHD, или ограниченная диссимиларность гессиана:\n",
        "\n",
        "$\n",
        "\\lVert \\nabla^2 f_i(x) - \\nabla^2 f(x) \\rVert \\leq \\delta, \\quad \\forall x.\n",
        "$\n",
        "\n",
        "Кроме того, $f_i$ является $\\delta$-слабо выпуклой, то есть $\\nabla^2 f_i(x) \\succeq -\\delta I$.\n",
        "\n",
        "Предположения из условия 1 и 2 ортогональны — возможно иметь $G = 0$ и $\\delta = 2\\beta$, или $\\delta = 0$, но $G > 1$."
      ],
      "metadata": {
        "id": "IZJthADfOFFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Основная идея статьи\n",
        "\n",
        "В статье рассказывается про новый алгоритм для федеративного обучения SCAFFOLD, доказывается его превосходство над прошлым решением - алгоритмом FEDAVG.\n",
        "\n",
        "FEDAVG стал алгоритмом выбора для федеративного обучения из-за своей простоты и низкой стоимости коммуникации. Однако, его производительность не полностью понята. Были получены точные скорости сходимости для FEDAVG и доказано, что он страдает от \"дрейфа клиента\"(модель, обученная на данных одного участника (клиента), показывает низкую эффективность или неустойчивость при применении к данным другого клиента), когда данные имеют различные характеристики, структуры или распределения в разных частях набора данных, что приводит к нестабильной и медленной сходимости.\n",
        "\n",
        "В статье же в качестве решения предлагается новый алгоритм SCAFFOLD, который использует управляемые переменные (снижение дисперсии), чтобы скорректировать \"дрейф клиента\" в его локальных обновлениях.\n",
        "\n",
        "Кроме того, в статье доказывается, что SCAFFOLD требует значительно меньше раундов коммуникации и не подвержен проблемам на данных, которые имеют различные характеристики, структуры или распределения в разных частях набора.\n",
        "\n",
        "Также SCAFFOLD для квадратичных функций может воспользоваться схожестью данных клиента, что приводит к еще более быстрой сходимости."
      ],
      "metadata": {
        "id": "s6CV4RnzSPXn"
      }
    }
  ]
}